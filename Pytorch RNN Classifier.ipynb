{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "from argparse import Namespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceDataset(Dataset):\n",
    "    def __init__(self, samples, longest, train_size=0.8):\n",
    "        super(VoiceDataset, self).__init__()\n",
    "        self.longest = longest\n",
    "        self.train_size = train_size\n",
    "        train_samples, test_samples = self.train_test_split(samples)\n",
    "        self.lookup_dict = {'train' : (train_samples, len(train_samples)), \n",
    "                           'test' : (test_samples, len(test_samples))}\n",
    "        self.set_split('train')\n",
    "        \n",
    "    def train_test_split(self, samples):\n",
    "        train_samples, test_samples = [], []\n",
    "        for num in samples:\n",
    "            mfccs = samples[num]\n",
    "            random.shuffle(mfccs)\n",
    "            size = len(mfccs)\n",
    "            train = mfccs[:int(size * self.train_size)]\n",
    "            train = [(vec, length, num) for vec, length in train]\n",
    "            test = mfccs[int(size * self.train_size):]\n",
    "            test = [(vec, length, num) for vec, length in test]\n",
    "            train_samples.extend(train)\n",
    "            test_samples.extend(test)\n",
    "        \n",
    "        return train_samples, test_samples\n",
    "    \n",
    "    def set_split(self, split='train'):\n",
    "        self.samples, self.length = self.lookup_dict[split]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        mfcc, length, num = self.samples[index]\n",
    "        num = int(num)-1\n",
    "        length = int(length)\n",
    "        return {'x_data' : mfcc,\n",
    "            'y_target' : num,\n",
    "               'x_len' : length}\n",
    "    \n",
    "    @classmethod\n",
    "    def create_dataset(cls, path):\n",
    "        data_path = os.path.join('Data', 'mfcc_samples.json')\n",
    "        f = open(data_path, 'r')\n",
    "        samples = json.load(f)\n",
    "        f.close()\n",
    "        longest = 0\n",
    "        for num in samples.keys():\n",
    "            for ind, mfcc in enumerate(samples[num]):\n",
    "                longest = max(longest, len(mfcc))\n",
    "                samples[num][ind] = np.asarray(mfcc)\n",
    "        \n",
    "        dim = samples['1'][0].shape[1]\n",
    "        \n",
    "        for num in samples.keys():\n",
    "            for ind, mfcc in enumerate(samples[num]):\n",
    "                length = len(mfcc)\n",
    "                to_pad = np.zeros((longest-length, dim))\n",
    "                samples[num][ind] = (np.vstack((mfcc, to_pad)), length)                \n",
    "        \n",
    "        return cls(samples, longest)\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size\n",
    "    \n",
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "        Generates batches using Pytorch's DataLoader and sets the device location to each tensor\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield data_dict\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_columns(y_out, x_lengths):\n",
    "    \"\"\"\n",
    "    Gets a vector, that's at the position indicated \n",
    "    by the corresponding value in x_lengths, from each batch datapoint in y_out.\n",
    "    \"\"\"\n",
    "    x_lengths = x_lengths.long().detach().cpu().numpy() - 1\n",
    "    out = []\n",
    "    for batch_index, column_index in enumerate(x_lengths):\n",
    "        out.append(y_out[batch_index, column_index])\n",
    "\n",
    "    return torch.stack(out)\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, hidden_size, dropout_rate=0.1, batch_first=True):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size=input_size, hidden_size=hidden_size, batch_first=batch_first)\n",
    "        self.linear1 = nn.Linear(in_features=hidden_size, out_features=32)\n",
    "        self.linear2 = nn.Linear(in_features=32, out_features=num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x_in, x_lengths=None, apply_softmax=False):\n",
    "        out, _ = self.rnn(x_in)\n",
    "        \n",
    "        if x_lengths is not None:\n",
    "            output = gather_columns(out, x_lengths)\n",
    "        else:\n",
    "            output = out[:, -1, :]\n",
    "            \n",
    "        output = self.linear1(output)\n",
    "        output = self.dropout(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.linear2(output)\n",
    "        if apply_softmax:\n",
    "            output = self.softmax(output)\n",
    "        return output\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    \"\"\"\n",
    "        Sets seed for reproducibility\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "def handle_dirs(dirpath):\n",
    "    \"\"\"\n",
    "        Creates directories\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda : False\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and path information\n",
    "    data_path=\"Data/mfcc_samples.json\",\n",
    "    save_dir=\"Model/\",\n",
    "    model_state_file=\"Model/model.pth\",\n",
    "    # Model hyper parameter\n",
    "    hidden_size = 64,\n",
    "    input_size = 32,\n",
    "    num_classes = 5,\n",
    "    # Training hyper parameter\n",
    "    num_epochs=20,\n",
    "    learning_rate=0.006,\n",
    "    batch_size=64,\n",
    "    early_stopping_criteria=3,\n",
    "    seed=1337,\n",
    "    # Runtime hyper parameter\n",
    "    cuda=False\n",
    ")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device('cuda' if args.cuda else 'cpu')\n",
    "print('Using cuda : {}'.format(args.cuda))\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "handle_dirs(args.save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'best_val': 1e8,\n",
    "            'iteration': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'test_loss': [],\n",
    "            'test_acc': [],\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    # Updates train state after each epoch, \n",
    "    # coordinates early stopping and saves the best model\n",
    "    if train_state['iteration'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        \n",
    "    elif train_state['iteration'] > 0:\n",
    "        last_loss, loss = train_state['test_loss'][-2:]\n",
    "\n",
    "        if last_loss < loss:\n",
    "            train_state['early_stopping_step'] += 1\n",
    "            train_state['early_stop'] = \\\n",
    "                train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "        else:\n",
    "            if loss < train_state['best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['best_val'] = loss\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "    return train_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VoiceDataset.create_dataset(path = args.data_path)\n",
    "model = RNN(input_size = args.input_size, num_classes = args.num_classes, hidden_size = args.hidden_size)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "train_state = make_train_state(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_split('train')\n",
    "train_len = dataset.__len__()\n",
    "dataset.set_split('test')\n",
    "test_len = dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, actual):\n",
    "    answers = np.argmax(output.detach().numpy(), axis=1)\n",
    "    correct = 0\n",
    "    for pred, ans in zip(actual, answers):\n",
    "        if pred == ans:\n",
    "            correct += 1\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1, train loss - 1.6108, test_loss - 1.6118\n",
      "Train Acc - 19.3 %  |  Test Acc - 19.54 %\n",
      "Epoch - 2, train loss - 1.6078, test_loss - 1.6082\n",
      "Train Acc - 20.6 %  |  Test Acc - 20.2 %\n",
      "Epoch - 3, train loss - 1.605, test_loss - 1.6107\n",
      "Train Acc - 21.89 %  |  Test Acc - 20.26 %\n",
      "Epoch - 4, train loss - 1.6107, test_loss - 1.607\n",
      "Train Acc - 19.71 %  |  Test Acc - 20.46 %\n",
      "Epoch - 5, train loss - 1.6052, test_loss - 1.6029\n",
      "Train Acc - 21.39 %  |  Test Acc - 21.19 %\n",
      "Epoch - 6, train loss - 1.5997, test_loss - 1.596\n",
      "Train Acc - 21.33 %  |  Test Acc - 23.7 %\n",
      "Epoch - 7, train loss - 1.4673, test_loss - 1.1863\n",
      "Train Acc - 32.98 %  |  Test Acc - 43.89 %\n",
      "Epoch - 8, train loss - 1.008, test_loss - 0.7317\n",
      "Train Acc - 57.86 %  |  Test Acc - 69.77 %\n",
      "Epoch - 9, train loss - 0.6171, test_loss - 0.4415\n",
      "Train Acc - 75.62 %  |  Test Acc - 80.46 %\n",
      "Epoch - 10, train loss - 0.3774, test_loss - 0.3457\n",
      "Train Acc - 86.04 %  |  Test Acc - 84.16 %\n",
      "Epoch - 11, train loss - 0.2312, test_loss - 0.2148\n",
      "Train Acc - 91.29 %  |  Test Acc - 89.77 %\n",
      "Epoch - 12, train loss - 0.1439, test_loss - 0.2029\n",
      "Train Acc - 94.3 %  |  Test Acc - 90.69 %\n",
      "Epoch - 13, train loss - 0.1222, test_loss - 0.1691\n",
      "Train Acc - 95.09 %  |  Test Acc - 91.62 %\n",
      "Epoch - 14, train loss - 0.1086, test_loss - 0.1501\n",
      "Train Acc - 95.37 %  |  Test Acc - 92.21 %\n",
      "Epoch - 15, train loss - 0.0825, test_loss - 0.1577\n",
      "Train Acc - 96.6 %  |  Test Acc - 92.28 %\n",
      "Epoch - 16, train loss - 0.0671, test_loss - 0.1798\n",
      "Train Acc - 97.19 %  |  Test Acc - 91.75 %\n",
      "Epoch - 17, train loss - 0.0683, test_loss - 0.0826\n",
      "Train Acc - 96.91 %  |  Test Acc - 94.85 %\n",
      "Epoch - 18, train loss - 0.0431, test_loss - 0.1018\n",
      "Train Acc - 97.97 %  |  Test Acc - 94.26 %\n",
      "Epoch - 19, train loss - 0.0538, test_loss - 0.1299\n",
      "Train Acc - 97.54 %  |  Test Acc - 93.6 %\n",
      "Epoch - 20, train loss - 0.0406, test_loss - 0.1034\n",
      "Train Acc - 98.08 %  |  Test Acc - 94.19 %\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['iteration'] = epoch_index\n",
    "\n",
    "        dataset.set_split('train')\n",
    "        model.train()\n",
    "        train_loss, correct_train = 0, 0\n",
    "        batches = generate_batches(dataset, args.batch_size)\n",
    "        num_batches = dataset.get_num_batches(args.batch_size)\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batches):\n",
    "            # Clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Compute output\n",
    "            output = model(x_in = batch_dict['x_data'].float(), x_lengths = batch_dict['x_len'])\n",
    "            correct_train += accuracy(output, batch_dict['y_target'])\n",
    "            # Calculate loss\n",
    "            loss = loss_function(output, batch_dict['y_target'])\n",
    "            train_loss += loss.item()\n",
    "            # Produce gradients based on loss\n",
    "            loss.backward()\n",
    "            # Update gradients\n",
    "            optimizer.step()\n",
    "\n",
    "        train_state['train_loss'].append(round((train_loss / num_batches), 4))\n",
    "\n",
    "        dataset.set_split('test')\n",
    "        model.eval()\n",
    "        test_loss, correct_test = 0, 0\n",
    "        batches = generate_batches(dataset, args.batch_size)\n",
    "        num_batches = dataset.get_num_batches(args.batch_size)\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batches):\n",
    "            output = model(x_in = batch_dict['x_data'].float(), x_lengths = batch_dict['x_len'])\n",
    "            correct_test += accuracy(output, batch_dict['y_target'])\n",
    "            loss = loss_function(output, batch_dict['y_target'])\n",
    "            test_loss += loss.item()\n",
    "\n",
    "        train_state['test_loss'].append(round((test_loss / num_batches), 4))\n",
    "\n",
    "        print('Epoch - {}, train loss - {}, test_loss - {}'.format(epoch_index+1, train_state['train_loss'][-1], \n",
    "                                                                   train_state['test_loss'][-1]))\n",
    "        train_state['train_acc'].append(round(correct_train / train_len * 100, 2))\n",
    "        train_state['test_acc'].append(round(correct_test / test_len * 100, 2))\n",
    "        print('Train Acc - {} %  |  Test Acc - {} %'.format(train_state['train_acc'][-1], \n",
    "                                                            train_state['test_acc'][-1]))\n",
    "\n",
    "        train_state = update_train_state(args, model, train_state)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

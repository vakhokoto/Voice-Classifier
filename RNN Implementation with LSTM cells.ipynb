{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, json, random\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # Data and path information\n",
    "    data_path=\"Data/mfcc_samples.json\",\n",
    "    save_dir=\"Model/\",\n",
    "    # Model hyper parameter\n",
    "    hidden_size = 64,\n",
    "    input_size = 32,\n",
    "    num_classes = 5,\n",
    "    # Training hyper parameter\n",
    "    num_epochs=100,\n",
    "    learning_rate=0.001,\n",
    "    seed=1337\n",
    ")\n",
    "\n",
    "args.input_size += args.hidden_size\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(args.data_path, 'r')\n",
    "samples = json.load(f)\n",
    "f.close()\n",
    "longest = 0\n",
    "for num in samples.keys():\n",
    "    for ind, mfcc in enumerate(samples[num]):\n",
    "        longest = max(longest, len(mfcc))\n",
    "        samples[num][ind] = np.asarray(mfcc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "s = set()\n",
    "for elem in samples:\n",
    "    print(elem)\n",
    "    for arr in samples[elem]:\n",
    "        if (len(arr) == 11):\n",
    "            print(elem)\n",
    "        s.add(len(arr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,

   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples, test_samples = [], []\n",
    "train_size = 0.8\n",
    "for num in samples:\n",
    "    mfccs = samples[num]\n",
    "    random.shuffle(mfccs)\n",
    "    size = len(mfccs)\n",
    "    train = mfccs[:int(size * train_size)]\n",
    "    train = [(vec, num) for vec in train]\n",
    "    test = mfccs[int(size * train_size):]\n",
    "    test = [(vec, num) for vec in test]\n",
    "    train_samples.extend(train)\n",
    "    test_samples.extend(test)\n",
    "\n",
    "random.shuffle(train_samples)\n",
    "random.shuffle(test_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Weights\n",
    "\n",
    "wf = np.random.randn(args.input_size, args.hidden_size) / np.sqrt(args.input_size / 2)\n",
    "wi = np.random.randn(args.input_size, args.hidden_size) / np.sqrt(args.input_size / 2)\n",
    "wc = np.random.randn(args.input_size, args.hidden_size) / np.sqrt(args.input_size / 2)\n",
    "wo = np.random.randn(args.input_size, args.hidden_size) / np.sqrt(args.input_size / 2)\n",
    "wy = np.random.randn(args.hidden_size, args.num_classes) / np.sqrt(args.hidden_size / 2)\n",
    "\n",
    "bf = np.zeros(args.hidden_size)\n",
    "bi = np.zeros(args.hidden_size)\n",
    "bc = np.zeros(args.hidden_size)\n",
    "bo = np.zeros(args.hidden_size)\n",
    "by = np.zeros(args.num_classes)\n",
    "\n",
    "# Initialize delta values\n",
    "\n",
    "dwf = np.zeros_like(wf)\n",
    "dwi = np.zeros_like(wi)\n",
    "dwc = np.zeros_like(wc)\n",
    "dwo = np.zeros_like(wo)\n",
    "dwy = np.zeros_like(wy)\n",
    "\n",
    "dbf = np.zeros_like(bf)\n",
    "dbi = np.zeros_like(bi)\n",
    "dbc = np.zeros_like(bc)\n",
    "dbo = np.zeros_like(bo)\n",
    "dby = np.zeros_like(by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(arr):\n",
    "    c = np.clip(arr, -700, 700) # float64 maximum expotentiable value\n",
    "    e = np.exp(c)\n",
    "    return e / np.sum(e, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy(out, label):\n",
    "    entropy = label * np.log(out + 1e-6) # to prevent log value overflow\n",
    "    return -np.sum(entropy, axis=1, keepdims=True)\n",
    "\n",
    "def sigmoid(arr):\n",
    "    c = np.clip(arr, -700, 700)\n",
    "    return 1 / (1 + np.exp(-c))\n",
    "\n",
    "def deriv_sigmoid(out):\n",
    "    return out * (1 - out)\n",
    "\n",
    "def tanh(arr):\n",
    "    c = np.clip(arr, -350, 350)\n",
    "    return 2 / (1 + np.exp(-2 * c)) - 1\n",
    "\n",
    "def deriv_tanh(out):\n",
    "    return 1 - np.square(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Cell(input_val):\n",
    "    batch_num = input_val.shape[1]\n",
    "    caches = []\n",
    "    states = []\n",
    "    states.append([np.zeros([batch_num, args.hidden_size]), np.zeros([batch_num, args.hidden_size])])\n",
    "    \n",
    "    for x in input_val:\n",
    "        c_prev, h_prev = states[-1]\n",
    "    \n",
    "        x = np.column_stack([x, h_prev])\n",
    "        hf = sigmoid(np.matmul(x, wf) + bf)\n",
    "        zd = np.matmul(x, wf)\n",
    "        hi = sigmoid(np.matmul(x, wi) + bi)\n",
    "        ho = sigmoid(np.matmul(x, wo) + bo)\n",
    "        hc = tanh(np.matmul(x, wc) + bc)\n",
    "    \n",
    "        c = hf * c_prev + hi * hc\n",
    "        h = ho * tanh(c)\n",
    "    \n",
    "        states.append([c, h])\n",
    "        caches.append([x, hf, hi, ho, hc])\n",
    "        \n",
    "    return caches, states\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sample):\n",
    "    \n",
    "    input_val = np.transpose(sample, [1, 0, 2])\n",
    "    \n",
    "    caches, states = LSTM_Cell(input_val)\n",
    "    c, h = states[-1]\n",
    "    \n",
    "    pred = softmax(np.dot(h, wy) + by)\n",
    "    label = np.argmax(pred)\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9\n",
      "Training Loss 1.64797\n",
      "Training Accuracy 20.27 %\n",
      "Iteration 19\n",
      "Training Loss 1.64264\n",
      "Training Accuracy 18.919 %\n",
      "Iteration 29\n",
      "Training Loss 1.64091\n",
      "Training Accuracy 20.27 %\n",
      "Iteration 39\n",
      "Training Loss 1.64119\n",
      "Training Accuracy 18.919 %\n",
      "Iteration 49\n",
      "Training Loss 1.64327\n",
      "Training Accuracy 17.568 %\n",
      "Iteration 59\n",
      "Training Loss 1.64759\n",
      "Training Accuracy 17.568 %\n",
      "Iteration 69\n",
      "Training Loss 1.64799\n",
      "Training Accuracy 14.865 %\n",
      "Iteration 79\n",
      "Training Loss 1.6408\n",
      "Training Accuracy 16.216 %\n",
      "Iteration 89\n",
      "Training Loss 1.64746\n",
      "Training Accuracy 16.216 %\n",
      "Iteration 99\n",
      "Training Loss 1.64902\n",
      "Training Accuracy 16.216 %\n"
     ]
    }
   ],
   "source": [
    "for i in range(args.num_epochs):\n",
    "    loss, correct = 0, 0\n",
    "    size = len(train_samples)\n",
    "    for sample, target in train_batched:\n",
    "        X = sample\n",
    "        Y = np.zeros((target.shape[0], args.num_classes))\n",
    "        target = [int(t)-1 for t in target]\n",
    "        for ind, num in enumerate(target):\n",
    "            Y[ind][num] = 1\n",
    "\n",
    "        Xt = np.transpose(X, [1, 0, 2])\n",
    "\n",
    "        caches, states = LSTM_Cell(Xt)\n",
    "        c, h = states[-1]\n",
    "        #print(h[3], h[5])\n",
    "        out = np.dot(h, wy) + by\n",
    "        pred = softmax(out)\n",
    "        loss += np.sum(cross_entropy(pred, Y))\n",
    "        predicted = np.argmax(pred, axis=1)\n",
    "        # calculate accuracy\n",
    "        correct += sum(predicted == target)\n",
    "        \n",
    "\n",
    "        # Backpropagation Through Time\n",
    "        dout = pred - Y\n",
    "        dwy = np.dot(h.T, dout)\n",
    "        dby = np.sum(dout, axis=0)\n",
    "\n",
    "        dc_next = np.zeros_like(c)\n",
    "        dh_next = np.zeros_like(h)\n",
    "\n",
    "        for t in range(Xt.shape[0]):\n",
    "            c, h = states[-t-1]\n",
    "            c_prev, h_prev = states[-t-2]\n",
    "\n",
    "            x, hf, hi, ho, hc = caches[-t-1]\n",
    "\n",
    "            tc = tanh(c)\n",
    "            dh = np.dot(dout, wy.T) + dh_next\n",
    "\n",
    "            dc = dh * ho * deriv_tanh(tc)\n",
    "            dc = dc + dc_next\n",
    "\n",
    "            dho = dh * tc \n",
    "            dho = dho * deriv_sigmoid(ho)\n",
    "\n",
    "            dhf = dc * c_prev \n",
    "            dhf = dhf * deriv_sigmoid(hf)\n",
    "\n",
    "            dhi = dc * hc \n",
    "            dhi = dhi * deriv_sigmoid(hi)\n",
    "\n",
    "            dhc = dc * hi \n",
    "            dhc = dhc * deriv_tanh(hc)\n",
    "\n",
    "            dwf += np.dot(x.T, dhf)\n",
    "            dbf += np.sum(dhf, axis=0)\n",
    "            dXf = np.dot(dhf, wf.T)\n",
    "\n",
    "            dwi += np.dot(x.T, dhi)\n",
    "            dbi += np.sum(dhi, axis=0)\n",
    "            dXi = np.dot(dhi, wi.T)\n",
    "\n",
    "            dwo += np.dot(x.T, dho)\n",
    "            dbo += np.sum(dho, axis=0)\n",
    "            dXo = np.dot(dho, wo.T)\n",
    "\n",
    "            dwc += np.dot(x.T, dhc)\n",
    "            dbc += np.sum(dhc, axis=0)\n",
    "            dXc = np.dot(dhc, wc.T)\n",
    "\n",
    "            dX = dXf + dXi + dXo + dXc\n",
    "\n",
    "            dc_next = hf * dc\n",
    "            dh_next = dX[:, -args.hidden_size:]\n",
    "\n",
    "            # Update weights        \n",
    "            \"\"\"\n",
    "            rame = args.learning_rate * dwf\n",
    "            rame = rame.tolist()\n",
    "            for ind, ram in enumerate(rame):\n",
    "                rame[ind] = [round(r, 5) for r in ram]\n",
    "            print(rame)\n",
    "            \"\"\"\n",
    "            wf -= args.learning_rate * dwf\n",
    "            wi -= args.learning_rate * dwi\n",
    "            wc -= args.learning_rate * dwc\n",
    "            wo -= args.learning_rate * dwo\n",
    "            wy -= args.learning_rate * dwy\n",
    "\n",
    "            bf -= args.learning_rate * dbf\n",
    "            bi -= args.learning_rate * dbi\n",
    "            bc -= args.learning_rate * dbc\n",
    "            bo -= args.learning_rate * dbo\n",
    "            by -= args.learning_rate * dby\n",
    "\n",
    "            # Initialize delta values\n",
    "            dwf *= 0\n",
    "            dwi *= 0\n",
    "            dwc *= 0\n",
    "            dwo *= 0\n",
    "            dwy *= 0\n",
    "\n",
    "            dbf *= 0\n",
    "            dbi *= 0\n",
    "            dbc *= 0\n",
    "            dbo *= 0\n",
    "            dby *= 0\n",
    "\n",
    "\n",
    "    if (i+1) % 10 == 0:\n",
    "        print('Iteration', i)\n",
    "        print('Training Loss', round(loss / size, 5))\n",
    "        print('Training Accuracy', round(correct / size * 100, 3), '%')\n",
    "    \"\"\"\n",
    "    loss, correct = 0, 0\n",
    "    size = len(test_samples)\n",
    "    for sample, target in test_samples:\n",
    "        X = np.expand_dims(sample, axis=0)\n",
    "        Y = np.zeros((args.num_classes,))\n",
    "        target = int(target) - 1\n",
    "        Y[target] = 1\n",
    "        Xt = np.transpose(X, [1, 0, 2])\n",
    "\n",
    "        caches, states = LSTM_Cell(Xt)\n",
    "        c, h = states[-1]\n",
    "\n",
    "        out = np.dot(h, wy) + by\n",
    "        pred = softmax(out)\n",
    "        loss += np.sum(cross_entropy(pred, Y))\n",
    "        predicted = np.argmax(pred)\n",
    "        \n",
    "    print('Validation Loss', round(loss / size, 5))\n",
    "    print('Validation Accuracy', round(correct / size * 100, 3), '%')\n",
    "    print('----------')\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
